{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Recognizing Traffic Signs Using Deep Learning\n",
    "### Author: Ashish Panchal (epababl03.ashishp@iima.ac.in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load/Import Libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local python modules\n",
    "sys.path.insert(0, os.path.abspath('../src/util'))\n",
    "sys.path.insert(1, os.path.abspath('../src/data'))\n",
    "\n",
    "from util import Util\n",
    "from dataloader import load_interim_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset\n",
    "The dataset used is:\n",
    "The German Traffic Sign Recognition Benchmark (GTSRB)\n",
    "\n",
    "### 2.1. GTSRB Dataset Overview\n",
    "For this project, we use a German traffic sign dataset from German Dataset, accessible through http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset#Annotationformat\n",
    "\n",
    "Dataset Overview Our training, validation, and testing dataset in pickle format. Each dataset contains number of images and itâ€™s label.\n",
    "\n",
    "Single-image, multi-class classification problem\n",
    "More than 40 classes\n",
    "More than 50,000 images in total\n",
    "Large, lifelike database\n",
    "Reliable ground-truth data due to semi-automatic annotation\n",
    "Physical traffic sign instances are unique within the dataset(i.e., each real-world traffic sign only occurs once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Signs Labels from signnames.csv\n",
    "util1 = Util()\n",
    "signs = util1.class_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/apanchal/EPABA/iima-epaba-batch03/data/interim/train.p\n",
      "Loading /Users/apanchal/EPABA/iima-epaba-batch03/data/interim/test.p\n",
      "Loading /Users/apanchal/EPABA/iima-epaba-batch03/data/interim/valid.p\n",
      "Loading /Users/apanchal/EPABA/iima-epaba-batch03/data/interim/train_plus_valid.p\n"
     ]
    }
   ],
   "source": [
    "#Load Image data\n",
    "interim_datasets = load_interim_data()\n",
    "train, test, valid = interim_datasets[0], interim_datasets[1], interim_datasets[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Dataset Summary and Exploratory\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "'features' is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "'labels' is a 2D array containing the label/class id of the traffic sign. The file signnames.csv contains id -> name mappings for each id.\n",
    "'sizes' is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "'coords' is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image.\n",
    "These coordinates assume the original image. The pickled data contains resized versions (32 by 32) of these images.\n",
    "\n",
    "The code snippets below will provide a basic summery of the Dataset.\n",
    "\n",
    "First, we will use numpy provide the number of images in each subset, in addition to the image size, and the number of unique classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "# Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# Number of testing examples\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# Number of validation examples.\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "# What's the shape of an traffic sign image?\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "# How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print('----- Complete Basic Data Summary ----- ')\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)\n",
    "print(\"Number of training examples: \", n_train)\n",
    "print(\"Number of testing examples: \", n_test)\n",
    "print(\"Number of validation examples: \", n_validation)\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
